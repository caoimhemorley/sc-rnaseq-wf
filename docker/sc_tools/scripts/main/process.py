import argparse
import scanpy as sc
import sys
import pandas as pd
from anndata import AnnData

sys.path.append("/opt/scripts/utility")
from helpers import score_cell_cycle, update_validation_metrics


def process_adata(
    adata: AnnData, 
    # markers: pd.DataFrame, 
    batch_key: str
) -> tuple[AnnData, pd.DataFrame, pd.DataFrame]:
    """
    do feature selection and add PCA
    """

    # does this work with sparse uint8?
    adata.layers["counts"] = adata.X.copy()  # type: ignore

    sc.pp.normalize_total(adata, target_sum=args.norm_target_sum)
    sc.pp.log1p(adata)

    # get cell cycle scores
    score_cell_cycle(adata, organism="human")

    # ### MAKE SURE MARKER GENES ARE KEPT
    # # defensive
    # markers = markers[~markers.index.duplicated(keep="first")].rename_axis(index=None)

    batch_key = args.batch_key
    # WARNING: using 'sample' can cause loess to fail in the highly_variable_genes function
    # HACK: using 'batch_id' instead of 'sample' for now
    if batch_key == "sample" or batch_key == "sample_id":
        print(f"WARNING: using 'batch_id' instead of '{batch_key}' for now")
        batch_key = "batch_id"

    hvgs_full = sc.experimental.pp.highly_variable_genes(
        adata,
        n_top_genes=args.n_top_genes,
        batch_key=batch_key,
        flavor="pearson_residuals",
        check_values=True,
        layer="counts",
        subset=False,
        inplace=False,
    )

    # # hack to make sure we keep the marker genes
    # hvgs_full.loc[markers.index, "highly_variable_nbatches"] = (
    #     hvgs_full["highly_variable_nbatches"].max() + 1.0
    # )
    # # Sort genes by how often they selected as hvg within each batch and
    # # break ties with median rank of residual variance across batches
    # hvgs_full.sort_values(
    #     ["highly_variable_nbatches", "highly_variable_rank"],
    #     ascending=[False, True],
    #     na_position="last",
    #     inplace=True,
    # )

    full_features = adata.var.copy()

    hvgs_full = hvgs_full.iloc[: args.n_top_genes].index.to_list()
    adata = adata[:, adata.var.index.isin(hvgs_full)]

    # add PCA of log1p normalized
    sc.pp.pca(adata, n_comps=args.n_comps)
    ## consider this alternate pca and avoid the normalization log1p?
    # scanpy.experimental.pp.normalize_pearson_residuals_pca(adata, *, theta=100, clip=None, n_comps=50, random_state=0, kwargs_pca=mappingproxy({}), mask_var=_empty, use_highly_variable=None, check_values=True, inplace=True)

    hvg_features = adata.var.copy()
    return (adata, full_features, hvg_features)


def main(args: argparse.Namespace):
    """
    basic logic with args as input

    """

    # Set CPUs to use for parallel computing
    sc._settings.ScanpyConfig.n_jobs = -1

    # 0. load data
    adata = sc.read_h5ad(args.adata_input)  # type: ignore
    # # 1. load marker_genes
    # # alternative way to get markers:
    # # https://github.com/NIH-CARD/brain-taxonomy/blob/main/markers/cellassign_card_markers.csv
    # markers = pd.read_csv(args.marker_genes, index_col=0)

    # 2. process data
    # adata, full_features, hvg_features = process_adata(adata, markers, args.batch_key)
    adata, full_features, hvg_features = process_adata(adata, args.batch_key)

    # 3. save the filtered adata
    # save the filtered adata
    adata.write_h5ad(filename=args.adata_output, compression="gzip")

    full_features.to_csv(args.full_gene_file, index=True)
    hvg_features.to_csv(args.hvg_file, index=True)

    # # 4. update the validation metrics
    # #######  validation metrics
    # val_metrics = pd.read_csv(args.output_validation_file, index_col=0)
    # output_metrics = update_validation_metrics(adata, "filter", val_metrics)
    # # log the validation metrics
    # output_metrics.to_csv(args.output_validation_file, index=True)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Normalize and identify features (hvg)"
    )
    parser.add_argument(
        "--adata-input",
        dest="adata_input",
        type=str,
        required=True,
        help="AnnData object for a dataset",
    )
    parser.add_argument(
        "--batch-key",
        dest="batch_key",
        type=str,
        required=True,
        help="Key in AnnData object for batch information",
    )
    parser.add_argument(
        "--adata-output",
        dest="adata_output",
        type=str,
        required=True,
        help="Output file to save AnnData object to",
    )
    parser.add_argument(
        "--norm-target-sum",
        type=int,
        required=True,
        help="The total count value that each cell will be normalized to [10000]",
    )
    parser.add_argument(
        "--n-top-genes",
        type=int,
        required=True,
        help="Number of HVG genes to keep [3000]",
    )
    parser.add_argument(
        "--n-comps",
        type=int,
        required=True,
        help="Number of principal components to compute [30]"
    )
    # parser.add_argument(
    #     "--marker-genes",
    #     dest="marker_genes",
    #     type=str,
    #     required=True,
    #     help="Path to marker_genes .csv file",
    # )
    parser.add_argument(
        "--output-all-genes",
        dest="full_gene_file",
        type=str,
        required=True,
        help="Output file to save feature metadata (full genes)",
    )
    parser.add_argument(
        "--output-hvg-genes",
        dest="hvg_file",
        type=str,
        required=True,
        help="Output file to save hvg metadata (full genes)",
    )


    args = parser.parse_args()
    main(args)
